"""
Pipeline for generating the summaries. 

how data is stored when the summary is created: 
    first level - /data/UKHLcorpus (cases)
                    /summarydata/UKHL_ - this is the relevant case name, 
                    updated with _feaetures at the end once the feature set is created

 to run with testing case numbers - need the UKHL corpus individual cases in a file called UKHL_corpus
 also need to duplicate and label it UKHL_corpus2 as it needs to be open twice at once - NEED TO FIX THIS
 
 nb - also need the 68txt_corpus and SUM_69_corpus folders in the /data folder as well as the corpus_list.csv 
 wordlist.csv and UKHL_corpus.csv - these files just stay in the /data file root
 
 also create a /summarydata folder, the files inside are generated by the code. 
 
 relevant dependencies are: 
    labelling.py
     tfidf_feature.py
     nvGroups.py
     ml.py
     summary.py 
     


   
    
    TODO: 
        HTML parser for cases not in the corpus, need to do the verb group and cue phrase matching first - which 
        would require retraining all the models based on the new feature set



@author: amyconroy
"""


class pipeline():
    def begin(self):
         print("Enter 1 to select a trained case number, anything else to a new case.")
         answer = input()
         if answer == '1':
             print("Enter the UKHL Corpus case number (1.19, 1.63, 1.68, 1.05, 1.02, 1.04, 1.35, 1.39, 1.38, 1.42, 1.34, 1.11, 1.15, 1.26,")
             print("1.28, 1.57, 1.43, 1.55, 2.13, 2.18, 2.3, 2.35, 2.34, 2.26, 2.24, 2.29, 2.21, 2.23, 2.45, 2.47, 2.41, 3.18, 3.21\n")
             print("3.22, 3.07, 3.1, 3.08, 3.02, 3.44, 3.41, 3.31, 3.32, 3.15, 3.14, 3.28, 1.03, 1.32, 1.7, 1.27, 2.25, 2.16, 2.06\n")
             print("2.02, 2.09, 2.03, 2.01, 2.08, 3.45, 3.36, 3.24, 3.17, 3.37, 3.03, 3.48, 3.42, 3.38\n")

             casenum = input()
             self.prepareCase(casenum)
         else:
             print("Enter the link of the case from Bailii")
             url = input()
             self.prepareLink(url)

             # here we would go and make it to the similar csv file, label, get ASMO, then follow same pipeline

    def prepareCase(self, casenum):

        print("\n PREPARING THE DATA FOR SUMMARISATION\n")
        import labelling
        labelling.labelling(casenum)
        import featureExtractor
        featureExtractor.featureExtractor(casenum)
        # integrated cue phrases above, now need to add cue phrases below 
        import ml
        ml.ml(casenum, True)
        print("\n SUMO PIPELINE SUMMARIES: \n")
        import summary
        summary.summary(casenum, True)
        print("\n")
        print("\n SUMO Summary Pipeline Complete.")

    def prepareLink(self, url):
        print("\n GET TEXT FROM THE LINK\n")
        import HTMLTextExtractor
        filepath = HTMLTextExtractor.HTMLTextExtractor(url).extract_text()
        casename = filepath.split("/")[-1].split(".")[0]

        import asmo_pipeline
        asmo_pipeline.asmo(filepath)

        import prepare_labelling
        prepare_labelling.prepare_labelling(filepath)

        import labelling
        labelling.labelling(casename)

        import featureExtractor
        featureExtractor.featureExtractor(casename)
        # integrated cue phrases above, now need to add cue phrases below
        import ml
        ml.ml(casename, True)

        print("\n SUMO PIPELINE SUMMARIES: \n")
        import summary
        summary.summary(casename, True)
        print("\n")
        print("\n SUMO Summary Pipeline Complete.")

pipeline = pipeline()
pipeline.begin()

    
    
    
